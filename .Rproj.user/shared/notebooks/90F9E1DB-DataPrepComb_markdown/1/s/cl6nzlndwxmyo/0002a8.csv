"0",""
"0",""
"0",""
"0","#################### LIGHT ###############################"
"0",""
"0","##### Stitch Light Data #####"
"0","#Packages I think I might perhaps maybe need..."
"0","library(ggpubr)"
"0","library(anytime)"
"0","library(googlesheets4)"
"0","library(ggpmisc)"
"0","library(plyr)"
"0",""
"0","library(dplyr)"
"0","library(lubridate)"
"0","library(tidyverse)"
"0","library(lubridate)"
"0","library(ggplot2)"
"0","library(scales)"
"0",""
"0","library(zoo)"
"0","library(xts)"
"0","library(forecast)"
"0","library(googledrive)"
"0","library(streamMetabolizer)"
"0","library(readr)"
"0",""
"0",""
"0","#### POKE 2019 ####"
"0","PAR.2019.url <- ""https://drive.google.com/drive/u/1/folders/11kWuwCF6pTXYolwYnxcsX6OcYyQeF35z"""
"0","PAR_19.prt1 <- drive_get(as_id(PAR.2019.url))"
"0","poke_par_glist <- drive_ls(PAR_19.prt1, pattern = ""191017_11619_POKE.CSV"")"
"1",""
"1"," "
"0","walk(poke_par_glist$id, ~ drive_download(as_id(.x), overwrite = TRUE))"
"1","File downloaded:
"
"1","[36m*[39m [36m191017_11619_POKE.CSV[39m [90m<id: 1gLqsUPh2N58BF_svKu03crpl-3daNkpS>[39m
"
"1","Saved locally as:
"
"1","[36m*[39m [34m191017_11619_POKE.CSV[39m
"
"0","poke.par.2019.Data <- read.csv(""191017_11619_POKE.CSV"","
"0","                               skip = 8, header = FALSE)"
"0","poke.par.2019.Data <- poke.par.2019.Data %>%"
"0","  dplyr::rename(ScanNumber = V1, Date = V2, Time = V3, RawValue = V4, CalibratedValue = V5)"
"0",""
"0","#Fix Date Time"
"0","poke.par.2019.Data$DateTime <- paste(poke.par.2019.Data$Date, poke.par.2019.Data$Time, sep="""")"
"0",""
"0","poke.par.2019.Data$DateTime <-  dmy_hms(poke.par.2019.Data$DateTime)"
"0","poke.par.2019.Data$DateTime <- force_tz(poke.par.2019.Data$DateTime, ""America/Anchorage"")"
"0",""
"0","#Calibrate logger 11619 to LICOR"
"0","poke.par.2019.Data$CalibratedValue <- poke.par.2019.Data$CalibratedValue * 0.035"
"0",""
"0",""
"0","#### VAUL 2019 ####"
"0","PAR.2019.url <- ""https://drive.google.com/drive/u/1/folders/11kWuwCF6pTXYolwYnxcsX6OcYyQeF35z"""
"0","PAR_19.prt1 <- drive_get(as_id(PAR.2019.url))"
"0","vaul_par_glist <- drive_ls(PAR_19.prt1, pattern = ""191017_11616_VAUL.CSV"")"
"1",""
"1"," "
"0","walk(vaul_par_glist$id, ~ drive_download(as_id(.x), overwrite = TRUE))"
"1","File downloaded:
"
"1","[36m*[39m [36m191017_11616_VAUL.CSV[39m [90m<id: 1MO_6VH7n0lI5ICg1uwnosw2it9Kwcxm0>[39m
"
"1","Saved locally as:
"
"1","[36m*[39m [34m191017_11616_VAUL.CSV[39m
"
"0","vaul.par.2019.Data <- read.csv(""191017_11616_VAUL.CSV"","
"0","                               skip = 8, header = FALSE)"
"0","vaul.par.2019.Data <- vaul.par.2019.Data %>%"
"0","  dplyr::rename(ScanNumber = V1, Date = V2, Time = V3, RawValue = V4, CalibratedValue = V5)"
"0",""
"0","#Fix Date Time"
"0","vaul.par.2019.Data$DateTime <- paste(vaul.par.2019.Data$Date, vaul.par.2019.Data$Time, sep="""")"
"0",""
"0","vaul.par.2019.Data$DateTime <-  dmy_hms(vaul.par.2019.Data$DateTime)"
"0","vaul.par.2019.Data$DateTime <- force_tz(vaul.par.2019.Data$DateTime, ""America/Anchorage"")"
"0",""
"0","#Calibrate logger 11616 to LICOR"
"0","vaul.par.2019.Data$CalibratedValue <- vaul.par.2019.Data$CalibratedValue * 0.032"
"0",""
"0",""
"0",""
"0","#### MOOS 2019 ####"
"0","PAR.2019.url <- ""https://drive.google.com/drive/u/1/folders/11kWuwCF6pTXYolwYnxcsX6OcYyQeF35z"""
"0","PAR_19.prt1 <- drive_get(as_id(PAR.2019.url))"
"0","moos_par_glist <- drive_ls(PAR_19.prt1, pattern = ""191022_11617_MOOS.CSV"")"
"1",""
"1"," "
"0","walk(moos_par_glist$id, ~ drive_download(as_id(.x), overwrite = TRUE))"
"1","File downloaded:
"
"1","[36m*[39m [36m191022_11617_MOOS.CSV[39m [90m<id: 1FaL-Dj5pI8zGgTE2A1lSKgAp9u3x99Iu>[39m
"
"1","Saved locally as:
"
"1","[36m*[39m [34m191022_11617_MOOS.CSV[39m
"
"0","moos.par.2019.Data <- read.csv(""191022_11617_MOOS.CSV"","
"0","                               skip = 8, header = FALSE)"
"0","moos.par.2019.Data <- moos.par.2019.Data %>%"
"0","  dplyr::rename(ScanNumber = V1, Date = V2, Time = V3, RawValue = V4, CalibratedValue = V5)"
"0",""
"0","#Fix Date Time"
"0","moos.par.2019.Data$DateTime <- paste(moos.par.2019.Data$Date, moos.par.2019.Data$Time, sep="""")"
"0",""
"0","moos.par.2019.Data$DateTime <-  dmy_hms(moos.par.2019.Data$DateTime)"
"0","moos.par.2019.Data$DateTime <- force_tz(moos.par.2019.Data$DateTime, ""America/Anchorage"")"
"0",""
"0","#Calibrate logger 11617 to LICOR"
"0","moos.par.2019.Data$CalibratedValue <- moos.par.2019.Data$CalibratedValue * 0.037 "
"0",""
"0",""
"0",""
"0","#### STRT 2019 ####"
"0","PAR.2019.url <- ""https://drive.google.com/drive/u/1/folders/11kWuwCF6pTXYolwYnxcsX6OcYyQeF35z"""
"0","PAR_19.prt1 <- drive_get(as_id(PAR.2019.url))"
"0","strt_par_glist <- drive_ls(PAR_19.prt1, pattern = ""191016_11620_PAR_STRT.CSV"")"
"1",""
"1"," "
"0","walk(strt_par_glist$id, ~ drive_download(as_id(.x), overwrite = TRUE))"
"1","File downloaded:
"
"1","[36m*[39m [36m191016_11620_PAR_STRT.CSV[39m [90m<id: 10IIz_Vn589DgKV0XrUPSc4dZr-I174gB>[39m
"
"1","Saved locally as:
"
"1","[36m*[39m [34m191016_11620_PAR_STRT.CSV[39m
"
"0","strt.par.2019.Data <- read.csv(""191016_11620_PAR_STRT.CSV"","
"0","                               skip = 8, header = FALSE)"
"0","strt.par.2019.Data <- strt.par.2019.Data %>%"
"0","  dplyr::rename(ScanNumber = V1, Date = V2, Time = V3, RawValue = V4, CalibratedValue = V5)"
"0",""
"0","#Fix Date Time"
"0","strt.par.2019.Data$DateTime <- paste(strt.par.2019.Data$Date, strt.par.2019.Data$Time, sep="""")"
"0",""
"0","strt.par.2019.Data$DateTime <-  dmy_hms(strt.par.2019.Data$DateTime)"
"0","strt.par.2019.Data$DateTime <- force_tz(strt.par.2019.Data$DateTime, ""America/Anchorage"")"
"0",""
"0","#Calibrate logger 11620 to LICOR"
"0","strt.par.2019.Data$CalibratedValue <- strt.par.2019.Data$CalibratedValue * 0.036"
"0",""
"0",""
"0","#early data"
"0","strt_par_glist2 <- drive_ls(PAR_19.prt1, pattern = ""190829_11620_PAR_STRT.CSV"")"
"1",""
"1"," "
"0","walk(strt_par_glist2$id, ~ drive_download(as_id(.x), overwrite = TRUE))"
"1","File downloaded:
"
"1","[36m*[39m [36m190829_11620_PAR_STRT.CSV[39m [90m<id: 1-rRqFFmeytsUvBKfXnVwABaczl6rNNy3>[39m
"
"1","Saved locally as:
"
"1","[36m*[39m [34m190829_11620_PAR_STRT.CSV[39m
"
"0","strt.par.2019.Data2 <- read.csv(""190829_11620_PAR_STRT.CSV"","
"0","                                skip = 8, header = FALSE)"
"0","strt.par.2019.Data2 <- strt.par.2019.Data2 %>%"
"0","  dplyr::rename(ScanNumber = V1, Date = V2, Time = V3, RawValue = V4, CalibratedValue = V5)"
"0",""
"0","#Fix Date Time"
"0","strt.par.2019.Data2$DateTime <- paste(strt.par.2019.Data2$Date, strt.par.2019.Data2$Time, sep="""")"
"0",""
"0","strt.par.2019.Data2$DateTime <-  dmy_hms(strt.par.2019.Data2$DateTime)"
"0","strt.par.2019.Data2$DateTime <- force_tz(strt.par.2019.Data2$DateTime, ""America/Anchorage"")"
"0",""
"0","#Calibrate logger 11620 to LICOR"
"0","strt.par.2019.Data2$CalibratedValue <- strt.par.2019.Data2$CalibratedValue * 0.036"
"0",""
"0",""
"0",""
"0","#other data / EXTRA"
"0","strt_par_glist3 <- drive_ls(PAR_19.prt1, pattern = ""190829_11618_PAR_STRT_EXTRA.CSV"")"
"1",""
"1"," "
"0","walk(strt_par_glist3$id, ~ drive_download(as_id(.x), overwrite = TRUE))"
"1","File downloaded:
"
"1","[36m*[39m [36m190829_11618_PAR_STRT_EXTRA.CSV[39m [90m<id: 1urXLxEq67p1M4uF-qQJn8J2RbvHMeYC8>[39m
"
"1","Saved locally as:
"
"1","[36m*[39m [34m190829_11618_PAR_STRT_EXTRA.CSV[39m
"
"0","strt.par.2019.Data3 <- read.csv(""190829_11618_PAR_STRT_EXTRA.CSV"","
"0","                                skip = 8, header = FALSE)"
"0","strt.par.2019.Data3 <- strt.par.2019.Data3 %>%"
"0","  dplyr::rename(ScanNumber = V1, Date = V2, Time = V3, RawValue = V4, CalibratedValue = V5)"
"0",""
"0","#Fix Date Time"
"0","strt.par.2019.Data3$DateTime <- paste(strt.par.2019.Data3$Date, strt.par.2019.Data3$Time, sep="""")"
"0",""
"0","strt.par.2019.Data3$DateTime <-  dmy_hms(strt.par.2019.Data3$DateTime)"
"0","strt.par.2019.Data3$DateTime <- force_tz(strt.par.2019.Data3$DateTime, ""America/Anchorage"")"
"0",""
"0","#Calibrate logger 11620 to LICOR"
"0","strt.par.2019.Data3$CalibratedValue <- strt.par.2019.Data3$CalibratedValue * 0.032"
"0",""
"0",""
"0","strt.par.2019.Data2 <- strt.par.2019.Data2 %>% filter(DateTime < ""2019-08-01 00:00:00"")"
"0",""
"0","strt.par.2019.Data3 <- strt.par.2019.Data3 %>% filter(DateTime >= ""2019-08-01 00:00:00"" & DateTime <= ""2019-09-01 00:00:00"")"
"0",""
"0",""
"0",""
"0","strt.par.2019.Data <- rbind(strt.par.2019.Data2, strt.par.2019.Data3, strt.par.2019.Data)"
"0",""
"0",""
"0",""
"0","#### FRCH 2019 ####"
"0","PAR.2019.url <- ""https://drive.google.com/drive/u/1/folders/11kWuwCF6pTXYolwYnxcsX6OcYyQeF35z"""
"0","PAR_19.prt1 <- drive_get(as_id(PAR.2019.url))"
"0","frch_par_glist <- drive_ls(PAR_19.prt1, pattern = ""191010_11615_FRCH.CSV"")"
"1",""
"1"," "
"0","walk(frch_par_glist$id, ~ drive_download(as_id(.x), overwrite = TRUE))"
"1","File downloaded:
"
"1","[36m*[39m [36m191010_11615_FRCH.CSV[39m [90m<id: 12bO_5urcswi2eLYE-wEcn42BrGMdR8R6>[39m
"
"1","Saved locally as:
"
"1","[36m*[39m [34m191010_11615_FRCH.CSV[39m
"
"0","frch.par.2019.Data <- read.csv(""191010_11615_FRCH.CSV"","
"0","                               skip = 8, header = FALSE)"
"0","frch.par.2019.Data <- frch.par.2019.Data %>%"
"0","  dplyr::rename(ScanNumber = V1, Date = V2, Time = V3, RawValue = V4, CalibratedValue = V5)"
"0",""
"0","#Fix Date Time"
"0","frch.par.2019.Data$DateTime <- paste(frch.par.2019.Data$Date, frch.par.2019.Data$Time, sep="""")"
"0",""
"0","frch.par.2019.Data$DateTime <-  dmy_hms(frch.par.2019.Data$DateTime)"
"0","frch.par.2019.Data$DateTime <- force_tz(frch.par.2019.Data$DateTime, ""America/Anchorage"")"
"0",""
"0","#Calibrate logger 11615 to LICOR"
"0","frch.par.2019.Data$CalibratedValue <- frch.par.2019.Data$CalibratedValue * 0.031"
"0",""
"0",""
"0",""
"0",""
"0",""
"0",""
"0",""
"0","##### 2020 #####"
"0",""
"0","#### POKE 2020 ####"
"0","PAR.2020.url <- ""https://drive.google.com/drive/u/1/folders/1udve7khOM2EX0l8BZQK3yehb2HUbwFtR"""
"0","PAR_2020.prt1 <- drive_get(as_id(PAR.2020.url))"
"0","poke_par_glist <- drive_ls(PAR_2020.prt1, pattern = ""11619_002_002_POKE_EndOfSeason.CSV"")"
"1",""
"1"," "
"0","walk(poke_par_glist$id, ~ drive_download(as_id(.x), overwrite = TRUE))"
"1","File downloaded:
"
"1","[36m*[39m [36m11619_002_002_POKE_EndOfSeason.CSV[39m [90m<id: 1udrRqV4sOYndIBBNGUpHlYoGuglGwAb5>[39m
"
"1","Saved locally as:
"
"1","[36m*[39m [34m11619_002_002_POKE_EndOfSeason.CSV[39m
"
"0","poke.par.2020.Data <- read.csv(""11619_002_002_POKE_EndOfSeason.CSV"","
"0","                               skip = 9, header = FALSE)"
"0","poke.par.2020.Data <- poke.par.2020.Data %>%"
"0","  dplyr::rename(ScanNumber = V1, Date = V2, Time = V3, RawValue = V4, CalibratedValue = V5)"
"0",""
"0","#Fix Date Time"
"0","poke.par.2020.Data$DateTime <- paste(poke.par.2020.Data$Date, poke.par.2020.Data$Time, sep="""")"
"0",""
"0","poke.par.2020.Data$DateTime <-  dmy_hms(poke.par.2020.Data$DateTime)"
"0","poke.par.2020.Data$DateTime <- force_tz(poke.par.2020.Data$DateTime, ""America/Anchorage"")"
"0",""
"0","#Calibrate logger 11619 to LICOR"
"0","poke.par.2020.Data$CalibratedValue <- poke.par.2020.Data$CalibratedValue * 0.035"
"0",""
"0",""
"0","#### VAUL 2020 ####"
"0","PAR.2020.url <- ""https://drive.google.com/drive/u/1/folders/1udve7khOM2EX0l8BZQK3yehb2HUbwFtR"""
"0","PAR_2020.prt1 <- drive_get(as_id(PAR.2020.url))"
"0","vaul_par_glist <- drive_ls(PAR_2020.prt1, pattern = ""11616_005_002_VAUL_EndOfSeason.CSV"")"
"1",""
"1"," "
"0","walk(vaul_par_glist$id, ~ drive_download(as_id(.x), overwrite = TRUE))"
"1","File downloaded:
"
"1","[36m*[39m [36m11616_005_002_VAUL_EndOfSeason.CSV[39m [90m<id: 1udckbrAK0LGZ0DhV3xAEpVduRBH9qicJ>[39m
"
"1","Saved locally as:
"
"1","[36m*[39m [34m11616_005_002_VAUL_EndOfSeason.CSV[39m
"
"0","vaul.par.2020.Data <- read.csv(""11616_005_002_VAUL_EndOfSeason.CSV"","
"0","                               skip = 9, header = FALSE)"
"0","vaul.par.2020.Data <- vaul.par.2020.Data %>%"
"0","  dplyr::rename(ScanNumber = V1, Date = V2, Time = V3, RawValue = V4, CalibratedValue = V5)"
"0",""
"0","#Fix Date Time"
"0","vaul.par.2020.Data$DateTime <- paste(vaul.par.2020.Data$Date, vaul.par.2020.Data$Time, sep="""")"
"0",""
"0","vaul.par.2020.Data$DateTime <-  dmy_hms(vaul.par.2020.Data$DateTime)"
"0","vaul.par.2020.Data$DateTime <- force_tz(vaul.par.2020.Data$DateTime, ""America/Anchorage"")"
"0",""
"0","#Calibrate logger 11616 to LICOR"
"0","vaul.par.2020.Data$CalibratedValue <- vaul.par.2020.Data$CalibratedValue * 0.032"
"0",""
"0",""
"0",""
"0","#### MOOS 2020 ####"
"0","PAR.2020.url <- ""https://drive.google.com/drive/u/1/folders/1udve7khOM2EX0l8BZQK3yehb2HUbwFtR"""
"0","PAR_2020.prt1 <- drive_get(as_id(PAR.2020.url))"
"0","moos_par_glist <- drive_ls(PAR_2020.prt1, pattern = ""11617_004_002_MOOS_EndOfSeason.CSV"")"
"1",""
"1"," "
"0","walk(moos_par_glist$id, ~ drive_download(as_id(.x), overwrite = TRUE))"
"1","File downloaded:
"
"1","[36m*[39m [36m11617_004_002_MOOS_EndOfSeason.CSV[39m [90m<id: 1w8u6FfA7wRDwm-BC6fkNMU_6aPqJ0pSF>[39m
"
"1","Saved locally as:
"
"1","[36m*[39m [34m11617_004_002_MOOS_EndOfSeason.CSV[39m
"
"0","moos.par.2020.Data <- read.csv(""11617_004_002_MOOS_EndOfSeason.CSV"","
"0","                               skip = 9, header = FALSE)"
"0","moos.par.2020.Data <- moos.par.2020.Data %>%"
"0","  dplyr::rename(ScanNumber = V1, Date = V2, Time = V3, RawValue = V4, CalibratedValue = V5)"
"0",""
"0","#Fix Date Time"
"0","moos.par.2020.Data$DateTime <- paste(moos.par.2020.Data$Date, moos.par.2020.Data$Time, sep="""")"
"0",""
"0","moos.par.2020.Data$DateTime <-  dmy_hms(moos.par.2020.Data$DateTime)"
"0","moos.par.2020.Data$DateTime <- force_tz(moos.par.2020.Data$DateTime, ""America/Anchorage"")"
"0",""
"0","#Calibrate logger 11617 to LICOR"
"0","moos.par.2020.Data$CalibratedValue <- moos.par.2020.Data$CalibratedValue * 0.037 "
"0",""
"0",""
"0",""
"0","#### STRT 2020 ####"
"0","PAR.2020.url <- ""https://drive.google.com/drive/u/1/folders/1udve7khOM2EX0l8BZQK3yehb2HUbwFtR"""
"0","PAR_2020.prt1 <- drive_get(as_id(PAR.2020.url))"
"0","strt_par_glist <- drive_ls(PAR_2020.prt1, pattern = ""11620_001_002_STRT_EndOfSeason.CSV"")"
"1",""
"1"," "
"0","walk(strt_par_glist$id, ~ drive_download(as_id(.x), overwrite = TRUE))"
"1","File downloaded:
"
"1","[36m*[39m [36m11620_001_002_STRT_EndOfSeason.CSV[39m [90m<id: 1tFeu5HbwlEvUq1WpSvB3EaBwF-8QixaY>[39m
"
"1","Saved locally as:
"
"1","[36m*[39m [34m11620_001_002_STRT_EndOfSeason.CSV[39m
"
"0","strt.par.2020.Data <- read.csv(""11620_001_002_STRT_EndOfSeason.CSV"","
"0","                               skip = 9, header = FALSE)"
"0","strt.par.2020.Data <- strt.par.2020.Data %>%"
"0","  dplyr::rename(ScanNumber = V1, Date = V2, Time = V3, RawValue = V4, CalibratedValue = V5)"
"0",""
"0","#Fix Date Time"
"0","strt.par.2020.Data$DateTime <- paste(strt.par.2020.Data$Date, strt.par.2020.Data$Time, sep="""")"
"0",""
"0","strt.par.2020.Data$DateTime <-  dmy_hms(strt.par.2020.Data$DateTime)"
"0","strt.par.2020.Data$DateTime <- force_tz(strt.par.2020.Data$DateTime, ""America/Anchorage"")"
"0",""
"0","#Calibrate logger 11620 to LICOR"
"0","strt.par.2020.Data$CalibratedValue <- strt.par.2020.Data$CalibratedValue * 0.036"
"0",""
"0",""
"0",""
"0","#### FRCH 2020 ####"
"0","PAR.2020.url <- ""https://drive.google.com/drive/u/1/folders/1udve7khOM2EX0l8BZQK3yehb2HUbwFtR"""
"0","PAR_2020.prt1 <- drive_get(as_id(PAR.2020.url))"
"0","frch_par_glist <- drive_ls(PAR_2020.prt1, pattern = ""11615_003_002_FRCH_2_EndOfSeason.CSV"")"
"1",""
"1"," "
"0","walk(frch_par_glist$id, ~ drive_download(as_id(.x), overwrite = TRUE))"
"1","File downloaded:
"
"1","[36m*[39m [36m11615_003_002_FRCH_2_EndOfSeason.CSV[39m [90m<id: 1wUbOR7grKA_HXp7okCwSs05GP0W3Xi91>[39m
"
"1","Saved locally as:
"
"1","[36m*[39m [34m11615_003_002_FRCH_2_EndOfSeason.CSV[39m
"
"0","frch.par.2020.Data <- read.csv(""11615_003_002_FRCH_2_EndOfSeason.CSV"","
"0","                               skip = 9, header = FALSE)"
"0","frch.par.2020.Data <- frch.par.2020.Data %>%"
"0","  dplyr::rename(ScanNumber = V1, Date = V2, Time = V3, RawValue = V4, CalibratedValue = V5)"
"0",""
"0","#Fix Date Time"
"0","frch.par.2020.Data$DateTime <- paste(frch.par.2020.Data$Date, frch.par.2020.Data$Time, sep="""")"
"0",""
"0","frch.par.2020.Data$DateTime <-  dmy_hms(frch.par.2020.Data$DateTime)"
"0","frch.par.2020.Data$DateTime <- force_tz(frch.par.2020.Data$DateTime, ""America/Anchorage"")"
"0",""
"0","#Calibrate logger 11615 to LICOR"
"0","frch.par.2020.Data$CalibratedValue <- frch.par.2020.Data$CalibratedValue * 0.031"
"0",""
"0",""
"0",""
"0",""
"0","###### 2021 #####"
"0","PAR.2021.url <- ""https://drive.google.com/drive/u/1/folders/1EPjHLDmfbCo5n12AKj7QpN2vXRCPQtg5"""
"0","PAR_2021.prt1 <- drive_get(as_id(PAR.2021.url))"
"0","par2021_glist <- drive_ls(PAR_2021.prt1, pattern = ""all.dates.par.2021.csv"")"
"1",""
"1"," "
"0","walk(par2021_glist$id, ~ drive_download(as_id(.x), overwrite = TRUE))"
"1","File downloaded:
"
"1","[36m*[39m [36mall.dates.par.2021.csv[39m [90m<id: 1hx6rsjmulZfZiLk5zDvFW0WNaaF7IbPt>[39m
"
"1","Saved locally as:
"
"1","[36m*[39m [34mall.dates.par.2021.csv[39m
"
"0","par2021.Data <- read.csv(""all.dates.par.2021.csv"","
"0","                         skip = 0, header = TRUE)"
"0",""
"0",""
"0","# Sort into sites"
"0","poke.par2021.Data <- par2021.Data %>% filter(site == ""poke"")"
"0","vaul.par2021.Data <- par2021.Data %>% filter(site == ""vaul"")"
"0","strt.par2021.Data <- par2021.Data %>% filter(site == ""strt"")"
"0","moos.par2021.Data <- par2021.Data %>% filter(site == ""moos"")"
"0","frch.par2021.Data <- par2021.Data %>% filter(site == ""frch"")"
"0",""
"0","poke.par2021.Data <- poke.par2021.Data %>%"
"0","  dplyr::rename(RawValue = V5)"
"0","vaul.par2021.Data <- vaul.par2021.Data %>%"
"0","  dplyr::rename(RawValue = V5)"
"0","strt.par2021.Data <- strt.par2021.Data %>%"
"0","  dplyr::rename(RawValue = V5)"
"0","moos.par2021.Data <- moos.par2021.Data %>%"
"0","  dplyr::rename(RawValue = V5)"
"0","frch.par2021.Data <- frch.par2021.Data %>%"
"0","  dplyr::rename(RawValue = V5)"
"0",""
"0","poke.par2021.Data$DateTime <- as.POSIXct(poke.par2021.Data$DateTime)"
"0","vaul.par2021.Data$DateTime <- as.POSIXct(vaul.par2021.Data$DateTime)"
"0","strt.par2021.Data$DateTime <- as.POSIXct(strt.par2021.Data$DateTime)"
"0","moos.par2021.Data$DateTime <- as.POSIXct(moos.par2021.Data$DateTime)"
"0","frch.par2021.Data$DateTime <- as.POSIXct(frch.par2021.Data$DateTime)"
"0",""
"0","#Calibrate loggers to LICOR"
"0","poke.par2021.Data$Calibrated.Value <- poke.par2021.Data$Calibrated.Value * 0.035"
"0",""
"0","vaul.par2021.Data$Calibrated.Value <- vaul.par2021.Data$Calibrated.Value * 0.032"
"0",""
"0","strt.par2021.Data$Calibrated.Value <- strt.par2021.Data$Calibrated.Value * 0.036"
"0",""
"0","moos.par2021.Data$Calibrated.Value <- moos.par2021.Data$Calibrated.Value * 0.037 "
"0",""
"0","frch.par2021.Data$Calibrated.Value <- frch.par2021.Data$Calibrated.Value * 0.031"
"0",""
"0",""
"0",""
"0",""
"0","#### Combine all years ####"
"0",""
"0","#POKE"
"0","poke.par2021.Data <- poke.par2021.Data %>%"
"0","  select(Calibrated.Value, DateTime)"
"0",""
"0","poke.par.2020.Data <- poke.par.2020.Data %>%"
"0","  select(CalibratedValue, DateTime) %>% dplyr::rename(Calibrated.Value = CalibratedValue)"
"0",""
"0","poke.par.2019.Data <- poke.par.2019.Data %>%"
"0","  select(CalibratedValue, DateTime) %>% dplyr::rename(Calibrated.Value = CalibratedValue)"
"0",""
"0","poke.combinded.par <- rbind(poke.par.2019.Data, poke.par.2020.Data, poke.par2021.Data)"
"0",""
"0","write.csv(poke.combinded.par,here(""outputs/poke.combinded.par.csv""), row.names = FALSE)"
"0",""
"0",""
"0","tiff(""Plots/Poke_PAR_all_years.tiff"", compression = ""lzw"", width = 1500, height =1000)"
