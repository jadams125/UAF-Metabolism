"0","############################# DEPTH #################################"
"0",""
"0","##### Stitch Depth #####"
"0","#Packages I think I might perhaps maybe need..."
"0","library(ggpubr)"
"0","library(anytime)"
"0","library(googlesheets4)"
"0","library(ggpmisc)"
"0","library(plyr)"
"0",""
"0","library(dplyr)"
"0","library(lubridate)"
"0","library(tidyverse)"
"0","library(lubridate)"
"0","library(ggplot2)"
"0","library(scales)"
"0","library(data.table)"
"0",""
"0","library(zoo)"
"0","library(xts)"
"0","library(forecast)"
"0","library(googledrive)"
"0","library(streamMetabolizer)"
"0","library(readr)"
"0",""
"0","#### 2019 ####"
"0",""
"0","#All PT data..."
"0","PT.2019.url <- ""https://drive.google.com/drive/u/1/folders/1VdtpYHtfxSqp2DRyWTCu4NorvQ5bx_i4"""
"0","pt.19.1 <- drive_get(as_id(PT.2019.url))"
"0","pt.19_glist <- drive_ls(pt.19.1, pattern = ""all.pt.2019.csv"")"
"1",""
"1"," "
"0","walk(pt.19_glist$id, ~ drive_download(as_id(.x), overwrite = TRUE))"
"1","File downloaded:
"
"1","[36m*[39m [36mall.pt.2019.csv[39m [90m<id: 11jxOk8mipXE-tihfuI50kGvMIQw99_Cv>[39m
"
"1","Saved locally as:
"
"1","[36m*[39m [34mall.pt.2019.csv[39m
"
"0","pt.2019.Data <- read.csv(""all.pt.2019.csv"","
"0","                         skip = 0, header = TRUE)"
"0",""
"0",""
"0","#separate out into individual sites to create a mean PT depth "
"0",""
"0","frch.data1 <- pt.2019.Data %>% filter(Site == ""FRCH1"")"
"0","frch.data2 <- pt.2019.Data %>% filter(Site == ""FRCH2"")"
"0","frch.2019.pt <- inner_join(frch.data1, frch.data2, by = ""DateTime"")"
"0","frch.2019.pt$DateTime <- as.POSIXct(paste(frch.2019.pt$DateTime), format = ""%Y-%m-%d %H:%M"", tz = ""America/Anchorage"")"
"0",""
"0","frch.2019.pt$AvgAbsDepth <- (frch.2019.pt$AbsPTDepth.x + frch.2019.pt$AbsPTDepth.y)/2"
"0",""
"0",""
"0","frch.2019.pt <- mutate(frch.2019.pt, AvgAbsDepth = ifelse(is.na(AvgAbsDepth) == TRUE, AbsPTDepth.y, AvgAbsDepth))"
"0",""
"0","frch.2019.pt <- mutate(frch.2019.pt, AvgAbsDepth = ifelse(is.na(AvgAbsDepth) == TRUE, AbsPTDepth.x, AvgAbsDepth))"
"0",""
"0",""
"0",""
"0","poke.data1 <- pt.2019.Data %>% filter(Site == ""POKE1"")"
"0","poke.data2 <- pt.2019.Data %>% filter(Site == ""POKE2"")"
"0","poke.2019.pt <- inner_join(poke.data1, poke.data2, by = ""DateTime"")"
"0","poke.2019.pt$DateTime <- as.POSIXct(paste(poke.2019.pt$DateTime), format = ""%Y-%m-%d %H:%M"", tz = ""America/Anchorage"")"
"0","poke.2019.pt$AvgAbsDepth <- (poke.2019.pt$AbsPTDepth.x + poke.2019.pt$AbsPTDepth.y)/2"
"0",""
"0",""
"0","poke.2019.pt <- mutate(poke.2019.pt, AvgAbsDepth = ifelse(is.na(AvgAbsDepth) == TRUE, AbsPTDepth.y, AvgAbsDepth))"
"0",""
"0","poke.2019.pt <- mutate(poke.2019.pt, AvgAbsDepth = ifelse(is.na(AvgAbsDepth) == TRUE, AbsPTDepth.x, AvgAbsDepth))"
"0",""
"0","#remove beaver dam points as per jake did in his discharge REPO"
"0",""
"0",""
"0","strt.data1 <- pt.2019.Data %>% filter(Site == ""STRT1"")"
"0","strt.data2 <- pt.2019.Data %>% filter(Site == ""STRT2"")"
"0","strt.data2$DateTime <- as.POSIXct(paste(strt.data2$DateTimeGMT), format = ""%Y-%m-%d %H:%M"", tz = ""America/Anchorage"")"
"0","strt.data2$DateTime <- as.character(strt.data2$DateTime)"
"0","strt.2019.pt <- merge(strt.data2, strt.data1, by = ""DateTime"", all = TRUE)"
"0","strt.data2$DateTime <- as.POSIXct(strt.data2$DateTime)"
"0","strt.2019.pt$AvgAbsDepth <- (strt.2019.pt$AbsPTDepth.x + strt.2019.pt$AbsPTDepth.y)/2"
"0",""
"0",""
"0","strt.2019.pt <- mutate(strt.2019.pt, AvgAbsDepth = ifelse(is.na(AvgAbsDepth) == TRUE, AbsPTDepth.y, AvgAbsDepth))"
"0",""
"0","strt.2019.pt <- mutate(strt.2019.pt, AvgAbsDepth = ifelse(is.na(AvgAbsDepth) == TRUE, AbsPTDepth.x, AvgAbsDepth))"
"0",""
"0","#MOOS and VAUL do not have a second PT in 2019"
"0","moos.data1 <- pt.2019.Data %>% filter(Site == ""MOOS1"")"
"0","moos.data1$DateTime <- as.POSIXct(paste(moos.data1$DateTimeGMT), format = ""%Y-%m-%d %H:%M"", tz = ""America/Anchorage"")"
"0","moos.2019.pt <- moos.data1"
"0",""
"0",""
"0",""
"0","vaul.data1 <- pt.2019.Data %>% filter(Site == ""VAUL1"")"
"0","vaul.data1$DateTime <- as.POSIXct(paste(vaul.data1$DateTimeGMT), format = ""%Y-%m-%d %H:%M"", tz = ""America/Anchorage"")"
"0","vaul.2019.pt <- vaul.data1"
"0",""
"0",""
"0",""
"0",""
"0","##### 2020 #####"
"0",""
"0",""
"0","PT.2020.url <- ""https://drive.google.com/drive/u/1/folders/1xYaOxYwRJQmYt0qOzahZMmTZRKW3lTVX"""
"0","pt.2020.1 <- drive_get(as_id(PT.2020.url))"
"0","pt.2020_glist <- drive_ls(pt.2020.1, pattern = ""all.pt.raw.csv"")"
"1",""
"1"," "
"0","walk(pt.2020_glist$id, ~ drive_download(as_id(.x), overwrite = TRUE))"
"1","File downloaded:
"
"1","[36m*[39m [36mall.pt.raw.csv[39m [90m<id: 103VsUkHb-MPphBLECBq2HXMzxQgbsNnq>[39m
"
"1","Saved locally as:
"
"1","[36m*[39m [34mall.pt.raw.csv[39m
"
"0","pt.2020.Data <- read.csv(""all.pt.raw.csv"","
"0","                         skip = 0, header = TRUE)"
"0",""
"0","pt.2020.Data <- pt.2020.Data %>% filter(Year == ""2020"")"
"0",""
"0",""
"0",""
"0","#separate out into individual sites to create a mean PT depth "
"0",""
"0","frch.data1.2020 <- pt.2020.Data %>% filter(Site == ""FRCH1"")"
"0","frch.data2.2020 <- pt.2020.Data %>% filter(Site == ""FRCH2"")"
"0","frch.2020.pt <- inner_join(frch.data1.2020, frch.data2.2020, by = ""DateTime"")"
"0","frch.2020.pt$DateTime <- as.POSIXct(paste(frch.2020.pt$DateTime), format = ""%Y-%m-%d %H:%M"", tz = ""America/Anchorage"")"
"0",""
"0","frch.2020.pt$AvgAbsDepth <- (frch.2020.pt$WaterLevel.x + frch.2020.pt$WaterLevel.y)/2"
"0",""
"0",""
"0","frch.2020.pt <- mutate(frch.2020.pt, AvgAbsDepth = ifelse(is.na(AvgAbsDepth) == TRUE, WaterLevel.y, AvgAbsDepth))"
"0",""
"0","frch.2020.pt <- mutate(frch.2020.pt, AvgAbsDepth = ifelse(is.na(AvgAbsDepth) == TRUE, WaterLevel.x, AvgAbsDepth))"
"0",""
"0",""
"0",""
"0","poke.data1.2020 <- pt.2020.Data %>% filter(Site == ""POKE1"")"
"0","poke.data2.2020 <- pt.2020.Data %>% filter(Site == ""POKE2"")"
"0","poke.2020.pt <- inner_join(poke.data1.2020, poke.data2.2020, by = ""DateTime"")"
"0","poke.2020.pt$DateTime <- as.POSIXct(paste(poke.2020.pt$DateTime), format = ""%Y-%m-%d %H:%M"", tz = ""America/Anchorage"")"
"0","poke.2020.pt$AvgAbsDepth <- (poke.2020.pt$WaterLevel.x + poke.2020.pt$WaterLevel.y)/2"
"0",""
"0",""
"0","poke.2020.pt <- mutate(poke.2020.pt, AvgAbsDepth = ifelse(is.na(AvgAbsDepth) == TRUE, WaterLevel.y, AvgAbsDepth))"
"0",""
"0","poke.2020.pt <- mutate(poke.2020.pt, AvgAbsDepth = ifelse(is.na(AvgAbsDepth) == TRUE, WaterLevel.x, AvgAbsDepth))"
"0",""
"0",""
"0",""
"0","strt.data1.2020 <- pt.2020.Data %>% filter(Site == ""STRT1"")"
"0","strt.data2.2020 <- pt.2020.Data %>% filter(Site == ""STRT2"")"
"0","strt.2020.pt <- inner_join(strt.data1.2020, strt.data2.2020, by = ""DateTime"")"
"0","strt.2020.pt$DateTime <- as.POSIXct(paste(strt.2020.pt$DateTime), format = ""%Y-%m-%d %H:%M"", tz = ""America/Anchorage"")"
"0","strt.2020.pt$AvgAbsDepth <- (strt.2020.pt$WaterLevel.x + strt.2020.pt$WaterLevel.y)/2"
"0",""
"0",""
"0","strt.2020.pt <- mutate(strt.2020.pt, AvgAbsDepth = ifelse(is.na(AvgAbsDepth) == TRUE, WaterLevel.y, AvgAbsDepth))"
"0",""
"0","strt.2020.pt <- mutate(strt.2020.pt, AvgAbsDepth = ifelse(is.na(AvgAbsDepth) == TRUE, WaterLevel.x, AvgAbsDepth))"
"0",""
"0",""
"0",""
"0",""
"0",""
"0","moos.data1.2020 <- pt.2020.Data %>% filter(Site == ""MOOS1"")"
"0","moos.data1$DateTime <- as.POSIXct(paste(moos.data1$DateTimeGMT), format = ""%Y-%m-%d %H:%M"", tz = ""America/Anchorage"")"
"0","moos.2020.pt <- moos.data1.2020"
"0",""
"0",""
"0",""
"0","vaul.data1.2020 <- pt.2020.Data %>% filter(Site == ""VAUL1"")"
"0","vaul.data1.2020$DateTime <- as.POSIXct(vaul.data1.2020$DateTime)"
"0","vaul.2020.pt <- vaul.data1.2020"
"0",""
"0","vaul.2020.pt$AvgAbsDepth <- vaul.2020.pt$WaterLevel"
"0",""
"0",""
"0","###### 2021 #####"
"0",""
"0","#VAUL "
"0","VAUL_PT_2021.url <- ""https://drive.google.com/drive/u/1/folders/1F80dynCpIo87e5EalwjNprze5UnLiomX"""
"0","vaul_pt.2021.1 <- drive_get(as_id(VAUL_PT_2021.url))"
"0","vaul.pt2021_glist <- drive_ls(vaul_pt.2021.1, pattern = ""vaul.pt.2021.csv"")"
"1",""
"1"," "
"0","walk(vaul.pt2021_glist$id, ~ drive_download(as_id(.x), overwrite = TRUE))"
"1","File downloaded:
"
"1","[36m*[39m [36mvaul.pt.2021.csv[39m [90m<id: 19BZ11QEJITzEcYp9mz0DQ4qXOaillENq>[39m
"
"1","Saved locally as:
"
"1","[36m*[39m [34mvaul.pt.2021.csv[39m
"
"0","vaul_PT_2021.Data <- read.csv(""vaul.pt.2021.csv"","
"0","                              skip = 0, header = TRUE)"
"0",""
"0",""
"0","vaul.2021.pt <- vaul_PT_2021.Data"
"0","# "
"0","# vaul.data1.2021 <- vaul_PT_2021.Data %>% filter(Site == ""VAUL1"")"
"0","# vaul.data2.2021 <- vaul_PT_2021.Data %>% filter(Site == ""VAUL2"")"
"0","# vaul.2021.pt <- inner_join(vaul.data1.2021, vaul.data2.2021, by = ""DateTime"")"
"0","# vaul.2021.pt$DateTime <- as.POSIXct(paste(vaul.2021.pt$DateTime), format = ""%Y-%m-%d %H:%M"", tz = ""America/Anchorage"")"
"0","# vaul.2021.pt$AvgAbsDepth <- (vaul.2021.pt$WaterLevel.x + vaul.2021.pt$WaterLevel.y)/2"
"0","# "
"0","# "
"0","# vaul.2021.pt <- mutate(vaul.2021.pt, AvgAbsDepth = ifelse(is.na(AvgAbsDepth) == TRUE, WaterLevel.y, AvgAbsDepth))"
"0","# "
"0","# vaul.2021.pt <- mutate(vaul.2021.pt, AvgAbsDepth = ifelse(is.na(AvgAbsDepth) == TRUE, WaterLevel.x, AvgAbsDepth))"
"0",""
"0",""
"0",""
"0",""
"0","#POKE"
"0",""
"0","POKE_PT_2021.url <- ""https://drive.google.com/drive/u/1/folders/1rOGiMGGMYzOoDcNQoJATxHARqR8Y1F1m"""
"0","poke_pt.2021.1 <- drive_get(as_id(POKE_PT_2021.url))"
"0","poke.pt2021_glist <- drive_ls(poke_pt.2021.1, pattern = ""poke.pt.2021.csv"")"
"1",""
"1"," "
"0","walk(poke.pt2021_glist$id, ~ drive_download(as_id(.x), overwrite = TRUE))"
"1","File downloaded:
"
"1","[36m*[39m [36mpoke.pt.2021.csv[39m [90m<id: 1ZGX7Y4cwBfo5vaSU8_ZRI-YrW3PoXvb3>[39m
"
"1","Saved locally as:
"
"1","[36m*[39m [34mpoke.pt.2021.csv[39m
"
"0","poke_PT_2021.Data <- read.csv(""poke.pt.2021.csv"","
"0","                              skip = 0, header = TRUE)"
"0",""
"0","poke.data1.2021 <- poke_PT_2021.Data %>% filter(Site == ""POKE1"")"
"0","poke.data2.2021 <- poke_PT_2021.Data %>% filter(Site == ""POKE2"")"
"0","poke.2021.pt <- inner_join(poke.data1.2021, poke.data2.2021, by = ""DateTime"")"
"0","poke.2021.pt$DateTime <- as.POSIXct(paste(poke.2021.pt$DateTime), format = ""%Y-%m-%d %H:%M"", tz = ""America/Anchorage"")"
"0","poke.2021.pt$AvgAbsDepth <- (poke.2021.pt$WaterLevel.x + poke.2021.pt$WaterLevel.y)/2"
"0",""
"0",""
"0","poke.2021.pt <- mutate(poke.2021.pt, AvgAbsDepth = ifelse(is.na(AvgAbsDepth) == TRUE, WaterLevel.y, AvgAbsDepth))"
"0",""
"0","poke.2021.pt <- mutate(poke.2021.pt, AvgAbsDepth = ifelse(is.na(AvgAbsDepth) == TRUE, WaterLevel.x, AvgAbsDepth))"
"0",""
"0",""
"0",""
"0","#FRCH"
"0",""
"0","FRCH_PT_2021.url <- ""https://drive.google.com/drive/u/1/folders/1pCn5m6WKsJv3ZzuWDvBr6iAp05bsGOWC"""
"0","frch_pt.2021.1 <- drive_get(as_id(FRCH_PT_2021.url))"
"0","frch.pt2021_glist <- drive_ls(frch_pt.2021.1, pattern = ""frch.pt.2021.csv"")"
"1",""
"1"," "
"0","walk(frch.pt2021_glist$id, ~ drive_download(as_id(.x), overwrite = TRUE))"
"1","File downloaded:
"
"1","[36m*[39m [36mfrch.pt.2021.csv[39m [90m<id: 1Kz-5Dyp5OWWIkGZkk1uJIyovT7XXvhki>[39m
"
"1","Saved locally as:
"
"1","[36m*[39m [34mfrch.pt.2021.csv[39m
"
"0","frch_PT_2021.Data <- read.csv(""frch.pt.2021.csv"","
"0","                              skip = 0, header = TRUE)"
"0",""
"0","frch.data1.2021 <- frch_PT_2021.Data %>% filter(Site == ""FRCH1"")"
"0","frch.data2.2021 <- frch_PT_2021.Data %>% filter(Site == ""FRCH2"")"
"0","frch.2021.pt <- inner_join(frch.data1.2021, frch.data2.2021, by = ""DateTime"")"
"0","frch.2021.pt$DateTime <- as.POSIXct(paste(frch.2021.pt$DateTime), format = ""%Y-%m-%d %H:%M"", tz = ""America/Anchorage"")"
"0",""
"0","frch.2021.pt$AvgAbsDepth <- (frch.2021.pt$WaterLevel.x + frch.2021.pt$WaterLevel.y)/2"
"0",""
"0",""
"0","frch.2021.pt <- mutate(frch.2021.pt, AvgAbsDepth = ifelse(is.na(AvgAbsDepth) == TRUE, WaterLevel.y, AvgAbsDepth))"
"0",""
"0","frch.2021.pt <- mutate(frch.2021.pt, AvgAbsDepth = ifelse(is.na(AvgAbsDepth) == TRUE, WaterLevel.x, AvgAbsDepth))"
"0",""
"0",""
"0",""
"0",""
"0","#MOOS"
"0",""
"0","MOOS_PT_2021.url <- ""https://drive.google.com/drive/u/1/folders/1Q14HB4khayh09dbTfFWGqHrkKaG1wXyN"""
"0","moos_pt.2021.1 <- drive_get(as_id(MOOS_PT_2021.url))"
"0","moos.pt2021_glist <- drive_ls(moos_pt.2021.1, pattern = ""moos.pt.2021.csv"")"
"1",""
"1"," "
"0","walk(moos.pt2021_glist$id, ~ drive_download(as_id(.x), overwrite = TRUE))"
"1","File downloaded:
"
"1","[36m*[39m [36mmoos.pt.2021.csv[39m [90m<id: 1jBJDMtN3ttjL9uHmF6o32gexMhAkO6Kj>[39m
"
"1","Saved locally as:
"
"1","[36m*[39m [34mmoos.pt.2021.csv[39m
"
"0","moos_PT_2021.Data <- read.csv(""moos.pt.2021.csv"","
"0","                              skip = 0, header = TRUE)"
"0",""
"0","moos.data1.2021 <- moos_PT_2021.Data %>% filter(Site == ""MOOS1"")"
"0","moos.data2.2021 <- moos_PT_2021.Data %>% filter(Site == ""MOOS2"")"
"0","moos.2021.pt <- inner_join(moos.data1.2021, moos.data2.2021, by = ""DateTime"")"
"0","moos.2021.pt$DateTime <- as.POSIXct(paste(moos.2021.pt$DateTime), format = ""%Y-%m-%d %H:%M"", tz = ""America/Anchorage"")"
"0","moos.2021.pt$AvgAbsDepth <- (moos.2021.pt$WaterLevel.x + moos.2021.pt$WaterLevel.y)/2"
"0",""
"0",""
"0","moos.2021.pt <- mutate(moos.2021.pt, AvgAbsDepth = ifelse(is.na(AvgAbsDepth) == TRUE, WaterLevel.y, AvgAbsDepth))"
"0",""
"0","moos.2021.pt <- mutate(moos.2021.pt, AvgAbsDepth = ifelse(is.na(AvgAbsDepth) == TRUE, WaterLevel.x, AvgAbsDepth))"
"0",""
"0",""
"0","#STRT"
"0",""
"0","STRT_PT_2021.url <- ""https://drive.google.com/drive/u/1/folders/1-om0nU42U8fNmeWtBrhM8WQTbGqBW8RN"""
"0","strt_pt.2021.1 <- drive_get(as_id(STRT_PT_2021.url))"
"0","strt.pt2021_glist <- drive_ls(strt_pt.2021.1, pattern = ""strt.pt.2021.csv"")"
"1",""
"1"," "
"0","walk(strt.pt2021_glist$id, ~ drive_download(as_id(.x), overwrite = TRUE))"
"1","File downloaded:
"
"1","[36m*[39m [36mstrt.pt.2021.csv[39m [90m<id: 11-gKNoBuwjhP7kbOcgl2gJCWQuS0NxUG>[39m
"
"1","Saved locally as:
"
"1","[36m*[39m [34mstrt.pt.2021.csv[39m
"
"0","strt_PT_2021.Data <- read.csv(""strt.pt.2021.csv"","
"0","                              skip = 0, header = TRUE)"
"0",""
"0","strt.data1.2021 <- strt_PT_2021.Data %>% filter(Site == ""STRT1"")"
"0","strt.data2.2021 <- strt_PT_2021.Data %>% filter(Site == ""STRT2"")"
"0","strt.2021.pt <- inner_join(strt.data1.2021, strt.data2.2021, by = ""DateTime"")"
"0","strt.2021.pt$DateTime <- as.POSIXct(paste(strt.2021.pt$DateTime), format = ""%Y-%m-%d %H:%M"", tz = ""America/Anchorage"")"
"0","strt.2021.pt$AvgAbsDepth <- (strt.2021.pt$WaterLevel.x + strt.2021.pt$WaterLevel.x)/2"
"0",""
"0","strt.2021.pt$AvgAbsDepth <- (strt.2021.pt$WaterLevel.x + strt.2021.pt$WaterLevel.y)/2"
"0",""
"0",""
"0","strt.2021.pt <- mutate(strt.2021.pt, AvgAbsDepth = ifelse(is.na(AvgAbsDepth) == TRUE, WaterLevel.y, AvgAbsDepth))"
"0",""
"0","strt.2021.pt <- mutate(strt.2021.pt, AvgAbsDepth = ifelse(is.na(AvgAbsDepth) == TRUE, WaterLevel.x, AvgAbsDepth))"
"0",""
"0",""
"0",""
"0","#Try and use discharge instead"
"0",""
"0","#MY METHOD"
"0","#MY METHOD"
"0",""
"0","#MY METHOD"
"0",""
"0","#MY METHOD"
"0","#MY METHOD"
"0",""
"0","#MY METHOD"
"0",""
"0",""
"0",""
"0","# Vaul 2019:"
"0",""
"0","#download flowmeter data"
"0","WR_19.url <- ""https://drive.google.com/drive/u/1/folders/1bm62_JO1dKrFPyUCz8E88K5q7IHElXPP"""
"0","WR.19.1 <- drive_get(as_id(WR_19.url))"
"0","vaul.wr19_glist <- drive_ls(WR.19.1, pattern = ""vaul_2019_flowmeter_Q_for_R_JAA.csv"")"
"1",""
"1"," "
"0","walk(vaul.wr19_glist$id, ~ drive_download(as_id(.x), overwrite = TRUE))"
"1","File downloaded:
"
"1","[36m*[39m [36mvaul_2019_flowmeter_Q_for_R_JAA.csv[39m [90m<id: 1MgCutKuTZeO_eKh-RIl-tko9Sotmvavj>[39m
"
"1","Saved locally as:
"
"1","[36m*[39m [34mvaul_2019_flowmeter_Q_for_R_JAA.csv[39m
"
"0","vaul_WR_19.Data <- read.csv(""vaul_2019_flowmeter_Q_for_R_JAA.csv"","
"0","                            skip = 1, header = TRUE, na.strings=c("""",""NA"",""blank""))"
"0",""
"0","vaul_WR_19.Data <- vaul_WR_19.Data %>% tidyr::fill(Date, .direction = (""down""))"
"0",""
"0","vaul_WR_19.Data <- vaul_WR_19.Data %>% tidyr::fill(Time, .direction = (""down""))"
"0",""
"0",""
"0","vaul_WR_19.Data$datetimeAK <- as.POSIXct(paste(vaul_WR_19.Data$Date, vaul_WR_19.Data$Time), format=""%m/%d/%Y %H:%M"")"
"0",""
"0",""
"0","vaul_WR_19.Data <- vaul_WR_19.Data %>%"
"0","  select(Depth..cm., datetimeAK)"
"0",""
"0","Vaul_depth_19_WR <- ddply(na.omit(vaul_WR_19.Data), .(datetimeAK), summarize, meanDepth = mean(as.numeric(Depth..cm.)))"
"0",""
"0","# vaul_wr.lm <- lm(meanDepth~datetimeAK, Vaul_depth_19)"
"0",""
"0",""
"0","Vaul_depth_19_WR <- setDT(Vaul_depth_19_WR)"
"0",""
"0","Vaul_depth_19_WR <- Vaul_depth_19_WR %>%"
"0","  dplyr::rename(datetimeAK = datetimeAK)"
"0",""
"0",""
"0",""
"0","vaul.2019.pt <- setDT(vaul.2019.pt)"
"0",""
"0",""
"0",""
"0","# vaul.2019.pt <- vaul.2019.pt %>%"
"0","#   mutate(across(c(AvgAbsDepth),"
"0","#                 ~ifelse(datetimeAK >= ""2019-08-26 02:00:00"" & datetimeAK <= ""2019-09-08 12:15:00"", NA, .)))"
"0",""
"0",""
"0",""
"0","vaul.2019.q.dt <- VAUL.2019.Q"
"0","vaul.2019.q.dt <- vaul.2019.q.dt %>%"
"0","  dplyr::rename(datetimeAK = DateTime)"
"0",""
"0","setDT(Vaul_depth_19_WR)"
"0","setDT(vaul.2019.q.dt)"
"0",""
"0","vaul.2019.q.dt$datetimeAK1 <- vaul.2019.q.dt$datetimeAK"
"0",""
"0","setkey( vaul.2019.q.dt, datetimeAK )"
"0","setkey( Vaul_depth_19_WR, datetimeAK )"
"0",""
"0","#WR was taken when EXO out of water. round depth point to nearest in data record"
"0","rounded.dates_vaul19 <- vaul.2019.q.dt[ Vaul_depth_19_WR, roll = ""nearest"" ]"
"0",""
"0","rounded.dates_vaul19_WR_Q <- rounded.dates_vaul19 %>% rename(discharge = Q) %>% select(datetimeAK, discharge, meanDepth)"
"0",""
"0","#convert to meters"
"0","rounded.dates_vaul19_WR_Q$meanDepth <- rounded.dates_vaul19_WR_Q$meanDepth /100"
"0",""
"0","vaul_pt_wr_graph <- ggplot(rounded.dates_vaul19_WR_Q, aes(discharge, meanDepth)) +"
"0","  geom_point()"
"0","# Add regression line"
"0","vaul_pt_wr_graph + geom_smooth(method = lm) + xlab(""Discharge"") +ylab (""Depth (WR)"")"
"2","`geom_smooth()` using formula 'y ~ x'
"
